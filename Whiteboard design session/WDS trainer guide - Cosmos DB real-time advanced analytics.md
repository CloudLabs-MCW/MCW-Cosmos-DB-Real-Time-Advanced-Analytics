![Microsoft Cloud Workshop](https://github.com/Microsoft/MCW-Template-Cloud-Workshop/raw/main/Media/ms-cloud-workshop.png 'Microsoft Cloud Workshop')

<div class="MCWHeader1">
Cosmos DB real-time advanced analytics
</div>

<div class="MCWHeader2">
Whiteboard design session trainer guide
</div>

<div class="MCWHeader3">
September 2021
</div>

Information in this document, including URL and other Internet Web site references, is subject to change without notice. Unless otherwise noted, the example companies, organizations, products, domain names, e-mail addresses, logos, people, places, and events depicted herein are fictitious, and no association with any real company, organization, product, domain name, e-mail address, logo, person, place or event is intended or should be inferred. Complying with all applicable copyright laws is the responsibility of the user. Without limiting the rights under copyright, no part of this document may be reproduced, stored in or introduced into a retrieval system, or transmitted in any form or by any means (electronic, mechanical, photocopying, recording, or otherwise), or for any purpose, without the express written permission of Microsoft Corporation.

Microsoft may have patents, patent applications, trademarks, copyrights, or other intellectual property rights covering subject matter in this document. Except as expressly provided in any written license agreement from Microsoft, the furnishing of this document does not give you any license to these patents, trademarks, copyrights, or other intellectual property.

The names of manufacturers, products, or URLs are provided for informational purposes only and Microsoft makes no representations and warranties, either expressed, implied, or statutory, regarding these manufacturers or the use of the products with any Microsoft technologies. The inclusion of a manufacturer or product does not imply endorsement of Microsoft of the manufacturer or product. Links may be provided to third party sites. Such sites are not under the control of Microsoft and Microsoft is not responsible for the contents of any linked site or any link contained in a linked site, or any changes or updates to such sites. Microsoft is not responsible for webcasting or any other form of transmission received from any linked site. Microsoft is providing these links to you only as a convenience, and the inclusion of any link does not imply endorsement of Microsoft of the site or the products contained therein.

Â© 2021 Microsoft Corporation. All rights reserved.

Microsoft and the trademarks listed at <https://www.microsoft.com/legal/intellectualproperty/Trademarks/Usage/General.aspx> are trademarks of the Microsoft group of companies. All other trademarks are property of their respective owners.

**Contents**

<!-- TOC -->

- [Trainer information](#trainer-information)
  - [Role of the trainer](#role-of-the-trainer)
  - [Whiteboard design session flow](#whiteboard-design-session-flow)
  - [Before the whiteboard design session: How to prepare](#before-the-whiteboard-design-session-how-to-prepare)
  - [During the whiteboard design session: Tips for an effective whiteboard design session](#during-the-whiteboard-design-session-tips-for-an-effective-whiteboard-design-session)
- [Cosmos DB real-time advanced analytics whiteboard design session student guide](#cosmos-db-real-time-advanced-analytics-whiteboard-design-session-student-guide)
  - [Abstract and learning objectives](#abstract-and-learning-objectives)
  - [Step 1: Review the customer case study](#step-1-review-the-customer-case-study)
    - [Customer situation](#customer-situation)
      - [Woodgrove's current process](#woodgroves-current-process)
    - [Customer needs](#customer-needs)
    - [Customer objections](#customer-objections)
    - [Infographic for common scenarios](#infographic-for-common-scenarios)
  - [Step 2: Design a proof of concept solution](#step-2-design-a-proof-of-concept-solution)
  - [Step 3: Present the solution](#step-3-present-the-solution)
  - [Wrap-up](#wrap-up)
  - [Additional references](#additional-references)
- [Cosmos DB real-time advanced analytics whiteboard design session trainer guide](#cosmos-db-real-time-advanced-analytics-whiteboard-design-session-trainer-guide)
  - [Step 1: Review the customer case study](#step-1-review-the-customer-case-study-1)
  - [Step 2: Design a proof of concept solution](#step-2-design-a-proof-of-concept-solution-1)
  - [Step 3: Present the solution](#step-3-present-the-solution-1)
  - [Wrap-up](#wrap-up-1)
  - [Preferred target audience](#preferred-target-audience)
  - [Preferred solution](#preferred-solution)
  - [Checklist of preferred objection handling](#checklist-of-preferred-objection-handling)
  - [Customer quote (to be read back to the attendees at the end)](#customer-quote-to-be-read-back-to-the-attendees-at-the-end)

<!-- /TOC -->

# Trainer information

Thank you for taking time to support the whiteboard design sessions as a trainer!

## Role of the trainer

An amazing trainer:

- Creates a safe environment in which learning can take place.

- Stimulates the participant's thinking.

- Involves the participant in the learning process.

- Manages the learning process (on time, on topic, and adjusting to benefit participants).

- Ensures individual participant accountability.

- Ties it all together for the participant.

- Provides insight and experience to the learning process.

- Effectively leads the whiteboard design session discussion.

- Monitors quality and appropriateness of participant deliverables.

- Effectively leads the feedback process.

## Whiteboard design session flow

Each whiteboard design session uses the following flow:

**Step 1: Review the customer case study (15 minutes)**

**Outcome**

Analyze your customer's needs.

- Customer's background, situation, needs and technical requirements

- Current customer infrastructure and architecture

- Potential issues, objectives and blockers

**Step 2: Design a proof of concept solution (60 minutes)**

**Outcome**

Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

- Determine your target customer audience.

- Determine customer's business needs to address your solution.

- Design and diagram your solution.

- Prepare to present your solution.

**Step 3: Present the solution (30 minutes)**

**Outcome**

Present solution to your customer:

- Present solution

- Respond to customer objections

- Receive feedback

**Wrap-up (15 minutes)**

- Review preferred solution

## Before the whiteboard design session: How to prepare

Before conducting your first whiteboard design session:

- Read the Student guide (including the case study) and Trainer guide.

- Become familiar with all key points and activities.

- Plan the point you want to stress, which questions you want to drive, transitions, and be ready to answer questions.

- Prior to the whiteboard design session, discuss the case study to pick up more ideas.

- Make notes for later.

## During the whiteboard design session: Tips for an effective whiteboard design session

**Refer to the Trainer guide** to stay on track and observe the timings.

**Do not expect to memorize every detail** of the whiteboard design session.

When participants are doing activities, you can **look ahead to refresh your memory**.

- **Adjust activity and whiteboard design session pace** as needed to allow time for presenting, feedback, and sharing.

- **Add examples, points, and stories** from your own experience. Think about stories you can share that help you make your points clearly and effectively.

- **Consider creating a "parking lot"** to record issues or questions raised that are outside the scope of the whiteboard design session or can be answered later. Decide how you will address these issues, so you can acknowledge them without being derailed by them.

**Have fun**! Encourage participants to have fun and share!

**Involve your participants.** Talk and share your knowledge but always involve your participants, even while you are the one speaking.

**Ask questions** and get them to share to fully involve your group in the learning process.

**Ask first**, whenever possible. Before launching into a topic, learn your audience's opinions about it and experiences with it. Asking first enables you to assess their level of knowledge and experience, and leaves them more open to what you are presenting.

**Wait for responses**. If you ask a question such as, "What's your experience with (fill in the blank)?" then wait. Do not be afraid of a little silence. If you leap into the silence, your participants will feel you are not serious about involving them and will become passive. Give participants a chance to think, and if no one answers, patiently ask again. You will usually get a response.

# Cosmos DB real-time advanced analytics whiteboard design session student guide

## Abstract and learning objectives

Woodgrove Bank, who provides payment processing services for commerce, is looking to design and implement a PoC of an innovative fraud detection solution. They want to provide new services to their merchant customers, helping them save costs by applying machine learning and advanced analytics to detect fraudulent transactions. Their customers are around the world, and the right solutions for them would minimize any latencies experienced using their service by distributing as much of the solution as possible, as closely as possible, to the regions in which their customers use the service.

In this whiteboard design session, you will work in a group to design the data pipeline PoC that could support the needs of Woodgrove Bank.

At the end of this workshop, you will be better able to design solutions that leverage the strengths of Cosmos DB in support of advanced analytics solutions that require high throughput ingest, low latency serving and global scale in combination with scalable machine learning, big data and real-time processing capabilities.

## Step 1: Review the customer case study

**Outcome**

Analyze your customer's needs.

Timeframe: 15 minutes

Directions: With all participants in the session, the facilitator/SME presents an overview of the customer case study along with technical tips.

1.  Meet your table participants and trainer.

2.  Read all of the directions for steps 1-3 in the student guide.

3.  As a table team, review the following customer case study.

### Customer situation

Woodgrove Bank, who provides payment processing services for commerce, is looking to design and implement a PoC of an innovative fraud detection solution. They know from experience and through contacts in the financial industry that there is a constant arms race between fraudsters and banks. Thanks to increasingly powerful and easily accessible technology, financial crime is on the rise. Payment processing companies, like Woodgrove Bank, and their merchant customers risk financial losses due to fraud.

They also risk fines from failing to detect or even prevent criminal acts like money laundering or terror financing. Woodgrove forecasts reaching over USD \$10 Billion in assets over the upcoming fiscal year, placing them within the stricter regulatory purview of institutions classified by the US government as "big banks". This means that they will be subject to regulatory fines over and above the fraud loss, putting their business at greater risk.

While all forms of fraud are on the rise, like ATM fraud, card transaction fraud, payment fraud, Woodgrove Bank would like to focus on online fraud. In the most basic terms, online fraud is committed when an unauthorized user impersonates another user by taking over their account, using malware, or hijacking internet sessions and uses the impersonated credentials to make purchase transactions. When dealing with millions of transactions, it is both crucial and challenging to detect and monitor fraud in real-time across all transactions. Doing so helps prevent additional losses and detect widespread attacks.

Given this focus on online fraud, they want to provide new services to their merchant customers, helping them save costs by applying machine learning and advanced analytics to detect fraudulent transactions. Their customers are around the world, and the right solutions for them would minimize any latencies experienced using their service by distributing as much of the solution as possible, as closely as possible, to the regions in which their customers use the service. This is the solution for which they would like to implement a PoC.

In flagging fraudulent transactions, they know there are tradeoffs between being overly aggressive and mistakenly identifying innocuous transactions as fraudulent, and not being aggressive enough such that they miss transactions that represent real fraud. According to Mari Stephens, Chief Information Officer (CIO), Woodgrove Bank, they would rather miss a fraudulent event in their automated system, than mistakenly identify innocuous transactions as fraudulent because the latter will frustrate both their merchant customer and the end customers and potentially lose their business. However, they want to balance this by doing as much as they can to detect fraud while minimizing the customer frustration. To address this, they believe the PoC will need to handle transactions at two "speeds". First, they want to screen transactions for fraud as they happen, only blocking a transaction if the system is very confident it is fraudulent. Second, they want to perform a more in-depth, offline fraud sweep of transactions to identify suspicious transactions. These are transactions which are potentially fraud, for which they will notify the merchant that they should perform additional verification with the end customer before completing the order. This deeper analysis that is performed in batch may use a slightly different ML model, but since it is a more intensive run, it needs to be run in batch and score transactions a little less leniently than the real-time scoring model. Remember, Woodgrove wants to minimize false positives during real-time scoring, but do a deeper analysis of the transactions later on and possibly tag those that were not blocked as suspicious.

They have decades worth of historical transaction data (including transactions identified as fraudulent) that they believe would be helpful in the fraud detection PoC. This data is in tabular format and can be exported to CSV files if needed.

The analysts at Woodgrove Bank are very interested in the recent notebook-driven approach to performing data science and data engineering tasks, and would prefer a solution that features notebooks as the standard way to explore data, prepare data, model, and define the logic for scheduled processing.

#### Woodgrove's current process

Woodgrove Bank provides a RESTful API that their merchant customers use to submit payments. The POC you design should not interrupt this process in any way. The solution you design needs to run side-by-side and augment their current process without changing their current workflow. Currently, as payments flow through their API endpoints, a series of cardholder verification steps are executed, such as matching the cardholder's billing address to their account. Once this validation check has completed, Woodgrove returns an authorization ID to the merchant, along with a status (accepted, rejected, declined, etc.). The payment details are entered into a relational database and the back-end payment process continues. There may be an opportunity to modify this process down the road, but that is not the focus of the POC.

The customer is asking for 2 additions to their current process:

- A RESTful API that can be called for immediate scoring on a transaction to see whether it should be blocked due to reasonably high-level confidence that it is fraudulent. Remember, this step should have a low number of false positives. The batch process that conducts a deeper sweep should flag suspicious transactions that were not blocked by this initial check.
- A real-time data ingestion pipeline they can pass data to at the time they save the payment transaction data from within their API. This should sit side-by-side with their current process, not change it.

To clarify, the requirement for real-time scoring of the payment transaction as fraudulent is not the same as the real-time ingest of all payment transaction data.

Below is a simple diagram Woodgrove Bank provided of their current process (blue boxes), showing where they would like you to fit in the new POC components (yellow boxes).

![Diagram provided by Woodgrove Bank showing their current process and where they would like you to add the new POC components.](media/woodgrove-bank-current-process.png "Woodgrove Bank's current process")

### Customer needs

1. Need to provide fraud detection services to our merchant customers, using incoming payment transaction data to provide early warning of fraudulent activity.

2. We would like to schedule offline scoring of âsuspicious activityâ using our trained model to create aggregates showing statistics around detected fraudulent activity, and make that data globally available in regions closest to our customers through our web applications.

3. For all transactions flowing through our system, we want to use our trained model to make near real-time predictions of fraudulent activity.

4. We want the ability to analyze all transactions over time, so we need to be able to store data from transaction sources into long-term storage, without interfering with jobs reading the data set.

5. We would like to use a standard platform that supports our near-term data pipeline needs while providing a long-term standard for data science, data engineering, and development.

### Customer objections

1. It's not clear to us if we can only use Cosmos DB as our web app's database, or if we should consider using it in other parts of our advanced analytics data pipeline such as for real-time transaction ingest or for serving of offline processed data.

2. Does Cosmos DB integrate with open source big data analytics like Apache Spark?

3. Properly selecting the right algorithm and training a model using the optimal set of parameters can take a lot of time. Is there a way to speed up this process?

4. We are concerned about how much it costs to use Cosmos DB for our solution. What is the real value of the service, and how do we set up Cosmos DB in an optimal way?

### Infographic for common scenarios

![Infographic for common scenarios](media/common-scenarios.png 'Common scenarios diagram')

## Step 2: Design a proof of concept solution

**Outcome**

Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

Timeframe: 60 minutes

**Business needs**

Directions: With all participants at your table, answer the following questions and list the answers on a flip chart:

1. Who should you present this solution to? Who is your target customer audience? Who are the decision makers?

2. What customer business needs do you need to address with your solution?

**Design**

Directions: With all participants at your table, respond to the following questions on a flip chart:

_High-level architecture_

1. Without getting into the details (the following sections will address the particular details), diagram your initial vision for handling the top-level requirements for payment fraud detection, including stream capture and processing, long-term storage, model training, global distribution of the model for real-time scoring and of the pre-scored fraud data, and dashboards.

_Globally distributed data_

1. Which data storage service would you recommend for storing the suspicious transactions? Remember, Woodgrove Bank wants to minimize access latency for their global customers. Be specific about how data is replicated.

2. How does your chosen service handle scaling to meet varying levels of demand across different regions? Can you set specific capacity for specific regions?

3. Distributed databases that replicate data to multiple locations have some potential delay between when you write a record and when that record is available for reading. What options does your chosen service have to ensure the data is not "stale" when read? Are there any tradeoffs between reducing the window between writes, and if so, how do they apply to Woodgrove Bank's situation?

_Data ingest_

1. What are your recommended options for ingesting payment transaction events as they occur in a scalable way that can be easily processed while maintaining event order with no data loss?

2. Of the ingest options you identified previously, which would you recommend for the scenario?

_Data pipeline processing_

1. Woodgrove Bank indicated that they would like a unified way to process both streaming data and batch data on a platform that can also support their data science, data engineering, and development needs. Which platform would you recommend, and why?

2. The big data systems Woodgrove Bank used in the past were only able to append new data to the end of existing data sets. This meant each time they had to update, they would actually create a duplicate row containing the changed data and then have to author queries to merge those rows so that they had a clean view of the current state of the data. How will your chosen platform cope with this challenge?

3. How will your chosen data processing platform connect to and process data from your chosen data ingest solution for streaming data?

4. What configuration would you need to apply to your solution to allow it to restart any stream processing in the case the job is stopped?

5. What specific secrets might their processing solution want to store? How would they securely store and access those secrets?

_Long-term data storage_

1. As incoming data is processed, refined, and scored, all of the transactions need to be persisted to long-term storage for analysis, model training and validation, and reporting. This storage needs to handle long-term growth, be fast enough to rapidly ingest new data while simultaneously handling reads against the same data set without interference, and act as a reliable data source for dashboards and reports. Which is your recommended long-term data storage solution, keeping in mind its role within your selected data pipeline processing platform?

_Model training and deployment_

1. Describe how your chosen data processing platform will support machine learning model training and deployment. The model will need to be trained on and validated against historical payment transaction data that includes known fraudulent transactions.

2. How will you schedule regular batch scoring of fraud data using the trained model, and make that data available to Woodgrove Bank's web applications at a global scale?

_Dashboards and reporting_

1. Woodgrove Bank's business analysts would like to have a set of dashboards they can monitor that provide real-time views of fraud trends at a global scale. Thinking back to how your proposed solution provides a set of summary tables containing business-level aggregates, what do you propose using to meet this requirement? Be specific about how this solution will be put in place and which features it supports.

2. Woodgrove Bank's data analysts, who build and maintain reports, are comfortable working with T-SQL. How can they efficiently access the data for analytical queries, ensuring they have access to the most up-to-date data, without impacting the transactional data store?

**Prepare**

Directions: With all participants at your table:

1.  Identify any customer needs that are not addressed with the proposed solution.

2.  Identify the benefits of your solution.

3.  Determine how you will respond to the customer's objections.

Prepare a 15-minute chalk-talk style presentation to the customer.

## Step 3: Present the solution

**Outcome**

Present a solution to the target customer audience in a 15-minute chalk-talk format.

Timeframe: 30 minutes

**Presentation**

Directions:

1.  Pair with another table.

2.  One table is the Microsoft team and the other table is the customer.

3.  The Microsoft team presents their proposed solution to the customer.

4.  The customer makes one of the objections from the list of objections.

5.  The Microsoft team responds to the objection.

6.  The customer team gives feedback to the Microsoft team.

7.  Tables switch roles and repeat Steps 2-6.

## Wrap-up

Timeframe: 15 minutes

Directions: Tables reconvene with the larger group to hear the facilitator/SME share the preferred solution for the case study.

## Additional references

| **Description**                                        | **Links**                                                                                                                         |
| ------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------- |
| Introduction to Cosmos DB                              | <https://docs.microsoft.com/azure/cosmos-db/introduction>                                                                         |
| About Event Hubs                                       | <https://docs.microsoft.com/azure/event-hubs/event-hubs-about>                                                                    |
| What is Azure Machine Learning?                | <https://docs.microsoft.com/azure/machine-learning/service/overview-what-is-azure-ml>                                             |
| What is Azure Machine Learning Studio? | <https://docs.microsoft.com/Awesomeazure/machine-learning/overview-what-is-machine-learning-studio> |
| What is Azure Key Vault?                               | <https://docs.microsoft.com/azure/key-vault/key-vault-overview>                                                                   |
| Scaling throughput in Azure Cosmos DB                  | <https://docs.microsoft.com/azure/cosmos-db/scaling-throughput>                                                                   |
| Partitioning and horizontal scaling in Azure Cosmos DB | <https://docs.microsoft.com/azure/cosmos-db/partition-data>                                                                       |
| Consistency levels in Azure Cosmos DB                  | <https://docs.microsoft.com/azure/cosmos-db/consistency-levels>                                                                   |
| Apache Spark Connector to Cosmos DB                    | <https://docs.microsoft.com/azure/cosmos-db/spark-connector>                                                                |
| What is Azure Cosmos DB analytical store? | <https://docs.microsoft.com/azure/cosmos-db/analytical-store-introduction?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json> |
| What is Azure Synapse Link for Azure Cosmos DB? | <https://docs.microsoft.com/azure/cosmos-db/synapse-link?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json> |
| Azure Cosmos DB autoscale throughput feature | <https://docs.microsoft.com/azure/cosmos-db/provision-throughput-autoscale> |

# Cosmos DB real-time advanced analytics whiteboard design session trainer guide

## Step 1: Review the customer case study

- Check in with your table participants to introduce yourself as the trainer.

- Ask, "What questions do you have about the customer case study?"

- Briefly review the steps and timeframes of the whiteboard design session.

- Ready, set, go! Let the table participants begin.

## Step 2: Design a proof of concept solution

- Check in with your tables to ensure that they are transitioning from step to step on time.

- Provide some feedback on their responses to the business needs and design.

  - Try asking questions first that will lead the participants to discover the answers on their own.

- Provide feedback for their responses to the customer's objections.

  - Try asking questions first that will lead the participants to discover the answers on their own.

## Step 3: Present the solution

- Determine which table will be paired with your table before Step 3 begins.

- For the first round, assign one table as the presenting team and the other table as the customer.

- Have the presenting team present their solution to the customer team.

  - Have the customer team provide one objection for the presenting team to respond to.

  - The presentation, objections, and feedback should take no longer than 15 minutes.

  - If needed, the trainer may also provide feedback.

## Wrap-up

- Have the table participants reconvene with the larger session group to hear the facilitator/SME share the following preferred solution.

## Preferred target audience

Mari Stephens, Chief Information Officer (CIO), Woodgrove Bank

The primary audience is the business decision makers and technology decision makers. From the case study scenario, this includes Mari Stephens, CIO for Woodgrove Bank. Usually, we talk to the infrastructure managers who report to the chief information officers (CIOs). We also speak with application sponsors (like a vice president \[VP\] line of business \[LOB\], or chief marketing officer \[CMO\]), or to those who represent the business unit IT or developers that report to application sponsors.

## Preferred solution

_High-level architecture_

1. Without getting into the details (the following sections will address the particular details), diagram your initial vision for handling the top-level requirements for payment fraud detection, including stream capture and processing, long-term storage, model training, global distribution of the model for real-time scoring and of the pre-scored fraud data, and dashboards.

    ![The Solution diagram is described in the text following this diagram.](media/outline-architecture.png "Solution diagram")

    The data flow for the solution begins with the payment transaction systems writing transactions to Azure Cosmos DB. Woodgrove Bank enables Synapse Link integration when provisioning the Azure Cosmos DB account. With this feature enabled, they turn on the analytical store when creating each of the containers, which serves as a fully isolated column store that is automatically populated when the payment transaction system writes data to the transactional container. The analytical store enables large-scale analytics against the operational data in Azure Cosmos DB, without impacting the transactional workloads or incurring resource unit (RU) costs. Woodgrove Bank's analysts query historical data within the analytical store and use it to join on reference data stored within the analytical store of other containers and the data lake. They execute these queries using Azure Synapse serverless Apache Spark pools and Azure Synapse serverless SQL pools.

    Woodgrove requires that the data retention for payment transactions stored in Azure Cosmos DB is set to 60 days and that all payment transactions need to be stored in long-term storage. To meet these requirements, the 'Transactional Store Time to Live (Transactional TTL)' property on the transactions container is enabled, and the TTL value is set to 60 on the documents. This setting automatically deletes payment transactions from the transactional store after the 60-day time period. The 'Analytical Store Time To Live (Analytical TTL)' setting allows Woodgrove Bank to manage the lifecycle of data retained in the analytical store independently from the transactional store. The TTL on the analytical store is set never to expire, enabling Woodgrove to seamlessly tier and define the two stores' data retention period.

    Azure Synapse Analytics serves as the end-to-end analytics platform that combines SQL data warehousing, big data analytics, and data integration, and is central to the architecture. Synapse Analytics is required when using the Synapse Link feature that enables the Azure Cosmos DB analytical store.

    Azure Machine Learning (Azure ML) is used to train both the real-time and batch machine learning models. The Azure ML workspace stores and manages trained models and deploys the trained model as a real-time scoring web service running on a highly available Azure Kubernetes Service cluster (AKS cluster). Woodgrove Bank uses the batch-scoring machine learning model within Azure Synapse Analytics notebooks to predict fraud against the day's transactions, build aggregates showing statistics around fraudulent activity, and write the results to an Azure Cosmos DB container. The batch-scoring model is also used within a Synapse notebook to reduce prediction latency by scoring the Azure Cosmos DB change feed's streaming data, using Spark Structured Streaming. All transactions with "suspicious activity" output are stored in Azure Cosmos DB, so it is globally available in regions closest to Woodgrove Bank's customers through their web applications. The analytical store feature is enabled on the container that contains predicted suspicious activity. Azure Synapse serverless SQL views are created against this and other analytical stores. Business analysts can access them using dashboards and reports in Power BI, which are embedded within the Synapse Analytics workspace. Data scientists and engineers can create their own reports against the Azure Cosmos DB analytical store, using Synapse notebooks.

    Finally, Azure Key Vault is used to securely store secrets, such as account keys and connection strings. The Synapse Linked Services securely access these secrets, hiding them from Synapse Analytics users who connect to the services.

    > **Note**: The preferred solution is only one of many possible, viable approaches.

_Globally distributed data_

1. Which data storage service would you recommend for storing the suspicious transactions? Remember, Woodgrove Bank wants to minimize access latency for their global customers. Be specific about how data is replicated.

    Azure Cosmos DB is well-suited for delivering large amounts of data in a fast and reliable way through data centers around the world. In addition, there is a connector available for reading and writing to Cosmos DB from Spark clusters, like those in Azure Synapse Analytics, making it a good candidate for both data ingest and as a data serving layer.

    It is a simple process to add or remove geographical regions associated with a Cosmos DB account at any time with a few clicks, or programmatically through a single API call. Because Cosmos DB automatically indexes the data stored within a Cosmos container upon ingestion, users can query the data without having to deal with a schema or the complications of index management in a globally distributed setup.

    When Woodgrove Bank configures Cosmos DB, they should take care to select an appropriate partition key for their data. They should select a partition key which provides even distribution of storage and throughput (measured in requests per second) at any given time to avoid storage and performance bottlenecks. For instance, they might choose to partition by account number or customer region. This key should be present in the bulk of queries for read-heavy scenarios to avoid excessive fan-out across numerous partitions. This is because each document with a specific partition key value belongs to the same logical partition, and is also stored in and served from the same physical partition. Each physical partition is replicated across geographical regions, resulting in global distribution.

2. How does your chosen service handle scaling to meet varying levels of demand across different regions? Can you set specific capacity for specific regions?

    In Azure Cosmos DB, provisioned throughput is represented as request units/second (RUs). RUs measure the cost of both read and write operations against your Cosmos DB container. Because Cosmos DB is designed with transparent horizontal scaling (e.g., scale out) and multi-master replication, you can very quickly and easily increase or decrease the number of RUs to handle thousands to hundreds of millions of requests per second around the globe with a single API call.

    Cosmos DB allows you to increment/decrement the RUs in small increments of 1000 at the database level, and in even smaller increments of 100 RU/s at the container level. It is recommended that you configure throughput at the container granularity for guaranteed performance for the container all the time, backed by SLAs. Other guarantees that Cosmos DB delivers are 99.999% read and write availability all around the world, with those reads and writes being served in less than 10 milliseconds at the 99th percentile.

    When you set a number of RUs for a container, Cosmos DB ensures that those RUs are available in all regions associated with your Cosmos DB account. When you scale out the number of regions by adding a new one, Cosmos will automatically provision the same quantity of RUs in the newly added region. You cannot selectively assign different RUs to a specific region. These RUs are provisioned for a container (or database) for all associated regions.

    To maximize analytical queries while minimizing RU cost, we use Azure Synapse Link for Cosmos DB and enable the analytical store on their Azure Cosmos DB containers. With this configuration, all transactional data is automatically stored in a fully isolated column store. This store enables large-scale analytics against the operational data in Azure Cosmos DB, without impacting the transactional workloads or incurring resource unit (RU) costs. Azure Synapse Link for Cosmos DB creates a tight integration between Azure Cosmos DB and Azure Synapse Analytics, which enables Woodgrove Bank to run near real-time analytics over their operational data with no-ETL and full performance isolation from their transactional workloads.

    To enable the Azure Cosmos DB analytical store, we use Azure Synapse Analytics. Azure Synapse is an end-to-end analytics platform which combines SQL data warehousing, big data analytics, and data integration into a single integrated environment.

    By combining the distributed scale of Cosmos DB's transactional processing with the built-in analytical store and the computing power of Azure Synapse Analytics, Azure Synapse Link enables a Hybrid Transactional/Analytical Processing (HTAP) architecture for optimizing Woodgrove Bank's business processes. This integration eliminates ETL processes, enabling business analysts, data engineers, and data scientists to self-serve and run near real-time BI, analytics, and Machine Learning pipelines over operational data.

3. Distributed databases that replicate data to multiple locations have some potential delay between when you write a record and when that record is available for reading. What options does your chosen service have to ensure the data is not "stale" when read? Are there any tradeoffs between reducing the window between writes, and if so, how do they apply to Woodgrove Bank's situation?

    Most distributed databases offer two consistency levels: strong and eventual. These live at different ends of a spectrum, where strong consistency often results in slower transactions because it synchronously writes data to each replica set. This guarantees that the reader will always see the most recent committed version of the data. Eventual consistency, on the other hand, asynchronously writes to each replica set with no ordering guarantee for reads. The replicas eventually converge, but the risk is that it can take several reads to retrieve the most up-to-date data.

    Azure Cosmos DB was designed with control over the tradeoffs between read consistency, availability, latency, and throughput. This is why Cosmos DB offers five consistency levels: strong, bounded staleness, session, consistent prefix, and eventual. As a general rule of thumb, you can get about 2x read throughput for session, consistent prefix, and eventual consistency models compared to bounded staleness or strong consistency.

    The Session consistency level is the default, and is suitable for most operations. It provides strong consistency for the session (application or connection), where all reads are current with writes from that session. Data from other sessions come in the correct order, but aren't guaranteed to be current. Session consistency level provides a balance of good performance and good availability at half the cost of both strong consistency and bounded staleness. As mentioned before, session provides about 2x read throughput compared to these two stronger consistency levels as well.

    One thing to consider in your design is that you may get stronger consistency guarantees in practice. Read-consistency is tied to the ordering and propagation of the write/update operations. If there are no writes being made to the data set, then the consistency level is not a factor. Put another way, if no new writes are being made to the data set at the same time the reads are happening, then all reads, regardless of consistency level, will show the latest data.

    In the case of Woodgrove Bank, Cosmos DB is being used for storing suspicious transactions that they identify by performing scheduled batch processing against all transactions. In this case, there are very few writes (which would happen in batch as the suspicious transactions are written out) compared to the number of reads (which might happen each time a customer reviews the flagged transactions). Because of this, for most of the read-heavy workload, all reads will get the latest data anyway. Therefore, a consistency level of Session will suffice for these documents, resulting in higher read throughput (approximately 2x faster) compared to strong and bounded staleness.

_Data ingest_

1. What are your recommended options for ingesting payment transaction events as they occur in a scalable way that can be easily processed while maintaining event order with no data loss?

    There are a couple of options in Azure for ingesting real-time streaming payment transactions. Which one you choose will depend on various factors, including the rate of flow (how many transactions/second), data source and compatibility, level of effort to implement, and long-term storage needs:

    1. **Event Hubs** is a Big Data streaming platform and event ingestion service, capable of ingesting millions of events per second. It supports multiple consumers (event processors), and is automatically scalable. Event Hubs is a good candidate for this architecture for the following reasons:

        - Fully managed PaaS with little configuration or management overhead.
        - Highly scalable to process millions of events per second. Use the Auto-inflate feature to automatically scale the number of throughput units to meet usage needs.
        - Contains an optional Apache Kafka endpoint, allowing for event processing from existing Kafka-based applications. This also allows for simple integration with Apache Spark clusters.
        - Simultaneously supports real-time and batch processing through Event Hubs Capture. This feature allows one to easily capture and store all events in their raw form to either Azure Blob storage or Azure Data Lake Store for long-term retention and micro-batch processing.
        - Event publishers (systems sending payment transaction data) can publish events using HTTPS, AMQP 1.0, or Apache Kafka 1.0 and above.
        - Event consumers can process streams using .NET, Java, Python, Go, or Node.js.

        Things to be cautious about are:

        - Event Hubs guarantees consistency and event ordering _per partition_. Since it is important to keep payment transactions in order when processing them, you can guarantee ordering by setting a partition key on the event, or use a `PartitionSender` object to only send events to a certain partition. This has scaling implications if you plan to use more than one partition. It also has uptime implications if the partition you are trying to send to is unavailable. If no partition is defined, then the data is distributed across available partitions. You may choose to ensure ordering by region or merchant, as an example, by creating a partition for the region or merchant. The potential downside to this approach is finite number of available partitions for a given event hub (32 max by default, higher via quota increase request). Another option is to aggregate events within the processing application by time-stamping the event with a custom sequence number. This requires state to be kept within the processing application.

    2. **Azure Cosmos DB** outputs a sorted list of documents that were changed in the order in which they were modified or inserted, as they are being written, by reading from its change feed. Like Event Hubs, the change feed output can be distributed across one or more consumers for parallel processing. Azure Cosmos DB is a good candidate for this architecture for the following reasons:

        - Fully managed PaaS with little configuration or management overhead.
        - Cosmos DB is highly scalable, and is already being used to store pre-scored fraud data.
        - An Apache Spark connector is available, allowing Azure Synapse Analytics Spark Pools to directly access the change feed with very little code.
        - Cosmos DB with change feed enabled acts as both a raw data store for batch processing and stream processing.
        - Event publishers can publish events to Cosmos DB using .NET, Java, Node.js, and Python, using a number of APIs, such as SQL, Cassandra, MongoDB, Gremlin, and Azure Table Storage.
        - The change feed feature can only be used by the SQL and Gremlin APIs of Cosmos DB. Woodgrove will be using the SQL API, so they will be able to use the change feed feature.
        - Cosmos DB is globally accessible across many Azure regions, bringing it closer to distributed event publishers and consumers.
        - In a multi-region Azure Cosmos account, if a write-region fails over, the change feed will work across the manual failover operation and it will be contiguous.
        - Coupled with Azure Synapse  Analytics, the Synapse Link feature enables the analytical store on the containers. The analytical store provides long-term storage and is optimized for analytical queries. These queries do not impact the RUs allocated to the Cosmos DB container's transactional store.
        - Cosmos DB Allows you to set a time-to-live (TTL) value, in seconds, on a container or on individual documents. This value tells Cosmos DB when to expire, or delete, the document(s) automatically. This setting can help save in storage costs by removing what you no longer need. Typically, this is used on hot data, or data that must be expired after a period of time due to regulatory requirements.
        - Cosmos DB analytical stores, enabled by Synapse Link, have a separate TTL setting. Usually, you would either disable TTL on the analytical store, or set it to -1 to never expire. This is because the analytical store's storage costs is significantly less over time.

        Things to be cautious about are:

        - Similar to Event Hubs, feed item ordering is guaranteed per logical partition key. There is no global guaranteed order across the partition key values. Also similar to Event Hubs, changes are available in parallel across all logical partition keys of a container, allowing for parallel processing by multiple consumers. As discussed in the previous section (globally distributed data), choosing an appropriate partition key for Cosmos DB is a critical step for ensuring balanced reads and writes, scaling, and, in this case, in-order change feed processing per partition. While there are no limits, per se, on the number of logical partitions, a single logical partition is allowed an upper limit of 10 GB of storage. Logical partitions cannot be split across physical partitions. For the same reason, if the partition key chosen is of bad cardinality, you could potentially have skewed storage distribution. For instance, if one logical partition becomes larger faster than the others and hits the maximum limit of 10 GB, while the others are nearly empty, the physical partition housing the maxed out logical partition cannot split and could cause an application downtime.

2. Of the ingest options you identified previously, which would you recommend for the scenario?

    While it is certainly possible to combine Azure Cosmos DB and Event Hubs for data ingest, this may result in unnecessary complexity, especially considering how closely their features (and challenges) align. For instance, it is possible to ingest all data into Cosmos DB and send events to Event Hubs through an intermediary event processor, such as Azure functions, in order to further process the event hub data downstream by consumers that can integrate with Event Hubs, but have no way to use Cosmos DB's change feed. However, this additional layer of abstraction is not necessary for the scenario of Woodgrove Bank. All things considered, the best approach is to select one after weighing the pros and cons of each. Remember, your decision should be based on the following factors: the rate of flow (how many transactions/second), data source and compatibility, level of effort to implement, and long-term storage needs.

    Both services are capable of acting as the ingestion service. They both support a high rate of flow, and both have options for long-term storage needs. However, Event Hubs requires an extra step for long-term storage, which is handled through Event Hubs Capture, whereas Cosmos DB already provides this storage for raw data. Considering the level of effort to implement, this favors Cosmos DB since transactional data is already being written to a database, those applications or APIs can be updated to also write to Cosmos DB, or switch over to Cosmos DB entirely as the single database. Event Hubs requires adding an event service to their architecture and learning how to use it from their current systems.

    Another point to consider is that Woodgrove Bank would like to ingest and serve data across multiple regions around the globe, and have that data synchronized as well. With Cosmos DB, you can simply add additional regions at any time either programmatically through its APIs, or through the "Replicate data globally" Cosmos DB settings in the portal. However, Event Hubs does not have this same option. You have to create new Event Hubs in each region you wish to send data to, modify your sending applications, as well as your stream processing applications to account for the additional service endpoints.

    A final decision point for using Cosmos DB for ingestion, is that it offers flexible message retention through its time-to-live (TTL) settings. This value can be set at the container level by applying a TTL value for all documents within, or on individual messages as they are sent. This optimization helps save in storage costs by automatically expiring (deleting) the documents after the specified period of time. For instance, you can set the TTL to 60 days to allow Woodgrove Bank to keep the streaming data available for that amount of time so they can reprocess the transactional data in Azure Synapse Analytics, or query the raw data within the container as needed. However, with the addition of the analytical store that is enabled through Synapse Link, all data gets automatically copied to a low-cost Azure storage account in columnar format, giving Woodgrove easy access to all their data, regardless of the transactional store's TTL value, from within Synapse Analytics. Since these queries are executed against the analytical store, they do not use any RU/s allocated to the transaction store.

    Event Hubs has a similar TTL feature called message retention. You can set the value between one and seven days, or a maximum of four weeks if you contact Microsoft support. Setting the TTL for documents saved to Cosmos DB individually for any length of time desired (even beyond 7 days) is an advantage Cosmos DB has over Event Hubs when used for ingesting streaming data.

_Data pipeline processing_

1. Woodgrove Bank indicated that they would like a unified way to process both streaming data and batch data on a platform that can also support their data science, data engineering, and development needs. Which platform would you recommend, and why?

    The recommended platform that meets these needs for this solution is Azure Synapse Analytics. When it comes to working with big data in a unified way, whether you process it real-time as it arrives or in batches, Apache Spark provides a fast and capable engine that also supports data science processes, like machine learning and advanced analytics. Azure Synapse brings together the best of SQL technologies used in enterprise data warehousing, Spark technologies used for big data, and Pipelines for data integration and ETL/ELT. Synapse has a web-based Studio that provides a single place for management, monitoring, coding, and security. Synapse features deep integration with other Azure services such as Power BI, Azure Cosmos DB, and Azure Machine Learning.

    Since the primary data source for this solution is Azure Cosmos DB, Synapse Analytics provides another big benefit: Azure Synapse Link for Azure Cosmos DB. Synapse Link for Azure Cosmos DB is a cloud-native hybrid transactional and analytical processing (HTAP) capability that will enable Woodgrove Bank to run near real-time analytics over operational data in Azure Cosmos DB. Azure Synapse Link creates a tight seamless integration between Azure Cosmos DB and Azure Synapse Analytics.

    Using the Azure Cosmos DB analytical store, a fully isolated column store, Azure Synapse Link enables no Extract-Transform-Load (ETL) analytics in Azure Synapse Analytics against the operational data at scale. Business analysts, data engineers, and data scientists can now use Synapse Spark or Synapse SQL interchangeably to run near real-time business intelligence, analytics, and machine learning pipelines. We can achieve this without impacting the performance of Woodgrove's transactional workloads on Azure Cosmos DB.

2. The big data systems Woodgrove Bank used in the past were only able to append new data to the end of existing data sets. This meant each time they had updates, they would actually create a duplicate row containing the changed data and then have to author queries to merge those rows so that they had a clean view of the current state of the data. How will your chosen platform cope with this challenge?

    The Azure Synapse Link for Azure Cosmos DB feature automatically handles updates to the underlying analytical store for each container on which the feature is enabled. Any inserts, updates, and deletes are automatically synchronized to the analytical store in near real-time. Also, for updates to the container's transactional store, Azure Cosmos DB supports upserts.

3. How will your chosen data processing platform connect to and process data from your chosen data ingest solution for streaming data?

    In our preferred architecture, we have chosen to ingest Woodgrove's data using Azure Cosmos DB and have enabled Synapse Link to integrate with Azure Synapse Analytics. The Azure Cosmos DB linked service enables Woodgrove to connect to the transactional Cosmos DB container and read from the stream provided by the change feed into a Spark DataFrame. You will use a Synapse Spark notebook to process the streaming data.

    Because the payment transactions will be arriving in real time, you will want to use Spark Structured Streaming to process the data. Think of a stream of data as a table to which data is continuously appended. You need to create a `foreachBatch` method that leverages the batch-scoring model for scoring microbatches. The function will write the scored results to a Cosmos DB container, using the linked service.

4. What configuration would you need to apply to your solution to allow it to restart any stream processing in the case the job is stopped?

    When defining a streaming query, one of the options that you need to specify is the location of a checkpoint directory.

    `spark.readStream.format("cosmos.oltp").option("spark.cosmos.changeFeed.checkpointLocation", "<path-to-checkpoint-directory>") ...`

    This is a structured streaming feature. It stores the current state of your streaming job.
    Should your streaming job stop for some reason and you restart it, it will continue from where it left off.

    Please note, if you do not have a checkpoint directory, when the streaming job stops, you lose all state around your streaming job and upon restart, you start from scratch.

5. What specific secrets might their processing solution want to store? How would they securely store and access those secrets?

    Specific secrets that they may need to be accessed by Azure Synapse Analytics are account names and keys for Azure Data Lake Storage, Cosmos DB connection strings or access keys, and Azure Machine Learning service account keys.

    To securely store these secrets, use Azure Key Vault and create a linked service within Synapse Analytics for the Key Vault account. Azure Key Vault provides a service that allows you to securely centralize application secrets. The benefit of it being a centralized store of secrets, is that you only need to define those secrets, like connection strings or account keys, in one place which can be accessed by several Azure services as well as custom applications.

_Long-term data storage_

1. As incoming data is processed, refined, and scored, all of the transactions need to be persisted to long-term storage for analysis, model training and validation, and reporting. This storage needs to handle long-term growth, be fast enough to rapidly ingest new data while simultaneously handling reads against the same data set without interference, and act as a reliable data source for dashboards and reports. Which is your recommended long-term data storage solution, keeping in mind its role within your selected data pipeline processing platform?

    Our proposed solution uses Synapse Analytics, which uses an Azure Data Lake Storage Gen2 (ADLS Gen2) account for its primary storage. ADLS Gen2 is a set of capabilities dedicated to big data analytics, built on Azure Blob storage. Data Lake Storage Gen2 is the result of converging the capabilities of Microsoft's two existing storage services, Azure Blob storage and Azure Data Lake Storage Gen1. Features from Azure Data Lake Storage Gen1, such as file system semantics, directory, and file level security and scale are combined with the low-cost, tiered storage, and high availability/disaster recovery capabilities from Azure Blob storage.

    ADLS Gen2 makes Azure Storage the foundation for building enterprise data lakes on Azure. Designed from the start to service multiple petabytes of information while sustaining hundreds of gigabits of throughput, ADLS Gen2 allows you to easily manage massive amounts of data.

    ADLS Gen2 allows you to manage and access data just as you would with a Hadoop Distributed File System (HDFS). The Azure Blob Filesystem (ABFS) driver allows Azure Synapse Analytics to access data stored in ADLS Gen2. This driver is optimized specifically for big data analytics, and overcomes the inherent deficiencies of the previous WASB driver.

    Since we are using the Azure Cosmos DB analytical store feature through Azure Synapse Link, all long-term analytical data is automatically stored within a separate, managed storage account. Azure Cosmos DB synchronizes data to this storage account as data in the transactional containers change. You do not need to access the storage account directly through Synapse Analytics or other means. Instead, you access the analytical store through Synapse Notebooks with Apache Spark, or through T-SQL through Synapse SQL Serverless (SQL on-demand). Synapse Link provides a layer of abstraction over top of the storage, letting you focus on writing queries instead of managing access to the storage account.

_Model training and deployment_

1. Describe how your chosen data processing platform will support machine learning model training and deployment. The model will need to be trained on and validated against historical payment transaction data that includes known fraudulent transactions.

    For model training and deployment, use Azure Machine Learning. Azure Machine Learning studio provides a web-based, interactive interface for managing the end-to-end machine learning lifecycle. Use Jupyter notebooks for model training and required data transformation steps as part of the ML pipeline. Create a datastore that points to the primary Synapse Analytics ADLS Gen2 account that contains the historical payment transaction data, which Woodgrove Bank said it can provide as a series of CSV files. The notebooks used for training the models can access the files through the datastore and transform that data as needed for cleanup and feature selection. A large portion of that data will be used for training, and the rest can be used to validate the performance of the trained model.

    The trained models are stored in Azure ML's model registry and deployed from a notebook, or through the Machine Learning studio's interface. Woodgrove will deploy the real-time scoring model to an Azure Kubernetes Service (AKS) cluster. Creating the AKS cluster is a one-time process for your workspace, where after you can reuse it for multiple deployments as the model gets updated through re-training. The basic steps are as follows:

    1. Register the model in the workspace model registry.

    2. Build a Docker image, including:

        - Download the registered model from the registry.
        - Create a dockerfile, with a Python environment based on the dependencies you specify in the environment yaml file.
        - Add your model files and the scoring script you supply in the dockerfile.
        - Build a new Docker image using the dockerfile.
        - Register the Docker image with the Azure Container Registry associated with the workspace.

    3. Deploy the Docker image to Azure Kubernetes Service (AKS).

    4. Start up a new container (or containers) in AKS.

    The above can be done through a series of scripts for automation. To deploy globally, modify the script to deploy to AKS clusters hosted within different regions.

    Ideally, Woodgrove will use [Azure Machine Learning's MLOps process](https://azure.microsoft.com/services/machine-learning/mlops/) in their production environment.

2. How will you schedule regular batch scoring of fraud data using the trained model, and make that data available to Woodgrove Bank's web applications at a global scale?

    The models that they have trained within Azure Machine Learning notebooks can be saved to ADLS Gen2 or to Azure Machine Learning model registry. These saved models can then be re-loaded by a Synapse Notebook to batch score and aggregate the daily âsuspicious transactionsâ results on a scheduled basis. Woodgrove will create a new pipeline in Synapse Analytics that contains a Notebook activity that calls the batch scoring notebook. They can configure the pipeline to run on a regular interval or as part of a larger data pipeline process. The notebook logic would use the Cosmos DB Linked Service to push the scored results out to the globally distributed set of containers, making the suspicious transactions âlocallyâ available to authorized consuming applications.

_Dashboards and reporting_

1. Woodgrove Bank's business analysts would like to have a set of dashboards they can monitor that provide real-time views of fraud trends at a global scale. Thinking back to how your proposed solution provides a set of summary tables containing business-level aggregates, what do you propose using to meet this requirement? Be specific about how this solution will be put in place and which features it supports.

    Since we are using the Azure Synapse Link for Azure Cosmos DB feature, and have enabled the analytical store on the Cosmos DB containers, we can create SQL views and add them to a Synapse SQL Serverless (SQL on-demand) database. These views use T-SQL to query the analytical store data, which does not impact the transactional store in any way, saving valuable RUs. Since the analytical store is optimized for read-heavy analytical queries, we can efficiently query over all current and historical data, create aggregates, join analytical stores together or with other external resources, and use these views to create Power BI reports and dashboards.

    The Power BI reports can be added to the Power BI service and embedded in Woodgrove's web applications for consumption by their processing customers. They can add the Power BI service workspace that contains the reports, as a Synapse linked service. Once added as a linked service, the Power BI datasets and reports can be managed from within Synapse Studio, providing a fully integrated experience.

2. Woodgrove Bank's data analysts, who build and maintain reports, are comfortable working with T-SQL. How can they efficiently access the data for analytical queries, ensuring they have access to the most up-to-date data, without impacting the transactional data store?

    Using the Azure Synapse Link for Azure Cosmos DB feature, Woodgrove can enable the analytical store on any container they create after enabling the feature. The analytical store is updated in near real-time and can be accessed using T-SQL through Synapse SQL Serverless (SQL on-demand) to create scripts or views.

## Checklist of preferred objection handling

1. It's not clear to us if we can only use Cosmos DB as our web app's database, or if we should consider using it in other parts of our advanced analytics data pipeline such as for real-time transaction ingest or for serving of offline processed data.

    Cosmos DB was created from the ground-up as a distributed database service that transparently handles the complexity of running within multiple regions around the world. Because providing data to customers around the globe is a key requirement, Cosmos DB is an ideal choice for ingesting data where it arrives, and serving the data where it is requested. It's major advantage when operating at a global scale is its high concurrency with low latency and predictable results. This combination is unique to Cosmos DB and ideal for Woodgrove Bank's needs. The change feed feature of Cosmos DB makes it useful for both storing raw transaction data as it is written, and notifying consumers, like Azure Synapse Analytics, of changes as they occur for real-time processing.

2. Does Cosmos DB integrate with open source big data analytics like Apache Spark?

    Yes, the `azure-cosmosdb-spark` connector can be used to read and write to Cosmos DB, and is also capable of using the change feed to react to events as they occur. Visit <https://github.com/Azure/azure-cosmosdb-spark> to learn how to use the connector and to access sample code. When using Azure Synapse Analytics, you can seamlessly access Cosmos DB from Synapse Notebooks after you create a Cosmos DB Linked Service. The Azure Synapse Link for Azure Cosmos DB feature adds further capability by providing access to the analytical store from within Synapse Analytics.

3. Properly selecting the right algorithm and training a model using the optimal set of parameters can take a lot of time. Is there a way to speed up this process?

    The typical approach to model training involves a time-consuming process trying dozens or even hundreds of combinations. Data scientists often try different ways of normalizing the data, different algorithms and different settings for those algorithms (hyperparameters). Data scientists will often setup a grid-search approach that will run multiple independent tests using differing combinations, measuring the performance of each and choosing the combination that provides the best results according to some performance metrics they select. With Azure Machine Learning AutoML, this search process is automated, and greatly simplifies the setup to try the typical combinations and quickly identify the best performing model against a user-selected performance metric. AutoML is used via the Azure Machine Learning Python SDK and can be utilized within Azure Machine Learning notebooks.

4. We are concerned about how much it costs to use Cosmos DB for our solution. What is the real value of the service, and how do we set up Cosmos DB in an optimal way?

    Azure Cosmos DB's greatest strength is that it provides a multi-model, globally available NoSQL database with high concurrency, low latency, and predictable results. The fact that it transparently synchronizes data to all regions, which can quickly and easily be added at any time, adds value by reducing the amount of development required to read and write the data and removes any need for synchronization.

    The cost of all database operations, such as throughput, CPU, and memory, is normalized by Azure Cosmos DB and is expressed in terms of Request Units (RUs). The cost to read a 1-KB item is 1 Request Unit (1 RU) and the minimum RUs required to consume 1 GB of storage is 40. The cost of writing a 1-KB item is 5 RUs. Many people risk over-allocating RUs to their containers when they may not need such high levels at all times. To save costs, a good tactic is to ramp up the number of RUs during batch processing or other read/write-heavy loads, then reduce the number of RUs afterwards. This can be done automatically or manually through the portal. An example of how this can be automatically done is to monitor Cosmos DB with Azure Monitor and set an alert rule that calls an Azure Function to scale up the number of RUs for that container. Then you would have another process to scale down as needed. You configure Azure Monitor to monitor the total requests for a 429 HTTP status code (which means the requests are throttled), apply alert logic where the total number of these codes are greater than a pre-defined value (like 10) over the last 5 minutes.

    ![A screenshot of Azure Monitor where the an alert is configured by selecting a status code value of 429 with a condition set to greater than, the time aggregation set to total, and threshold set to 10.](media/azure-monitor.png 'Azure Monitor')

    Another option is to use the [autoscale throughput feature](https://docs.microsoft.com/azure/cosmos-db/provision-throughput-autoscale) on containers and databases. With autoscale, Azure Cosmos DB automatically and instantly scales the throughput (RU/s) of your database or container based on usage, without impacting the availability, latency, throughput, or performance of the workload.

    Cosmos DB also offers flexible time-to-live (TTL) that can be set at the container-level or on individual documents. This value tells Cosmos DB to expire, or delete, a document after a certain amount of seconds. This value can be set to as little as one second, or months into the future. As a result, you can save storage costs for records that are no longer needed. The flexibility of setting TTL at the container or record-level is a unique characteristic of Cosmos DB.

    If you plan on using Cosmos DB for analytical queries, you should use the Azure Synapse Link for Azure Cosmos DB feature. This feature allows you to use an analytical store for your containers. With this configuration, all transactional data is automatically stored in a fully isolated column store. This store enables large-scale analytics against the operational data in Azure Cosmos DB, without impacting the transactional workloads or incurring resource unit (RU) costs.

    Another pitfall that developers or database administrators experience, especially those who are used to using traditional relational databases, is that they tend to want to create a Cosmos DB container for each "table", or entity type they wish to store. Instead, you should consider storing any number of entity types within the same container. The reason for this is that there is a cost associated with each container that you add. Because containers do not enforce any type of schema, you are able to store entities of the same type with different schemas (likely due to changes over time or from excluding properties with no values), or entirely different entities within that container. A common approach to dealing with different types of entities within a container is to add a string property like `collectionType` so that you can filter query results by that type. For instance, Woodgrove Bank stores transaction and suspicious activity data within the same container. They could assign a value of "Transaction" to the transaction entities, and "SuspiciousActivity" to the suspicious activity entities. Both types and many others can coexist within the container and can easily be filtered by the `collectionType` property.

## Customer quote (to be read back to the attendees at the end)

"As a bank who is entrusted by customers all around the world with processing online payments, we have to be pro-active in detecting online fraud and protecting those customers. If we do not detect such activity quickly enough, things can spiral out of control, causing losses and in both monetary terms and our customers' trust. Azure has really enabled us to add that layer of security and peace of mind at a grand scale."

--- Mari Stephens, CIO, Woodgrove Bank
